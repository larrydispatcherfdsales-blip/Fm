# ===== fmcsa-post-merge.yml (Updated) =====
name: FMCSA Post Merge

on:
  workflow_dispatch: # Ab yeh manually ya doosre workflow se trigger hoga

jobs:
  process:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: pip install pandas requests

      - name: Download & Merge All Artifacts
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          RUN_ID: ${{ github.run_id }}
        run: |
          python - <<'EOF'
          import os, requests, zipfile, glob
          from io import BytesIO
          import pandas as pd

          repo = os.getenv("GITHUB_REPOSITORY")
          token = os.getenv("GITHUB_TOKEN")
          current_run_id = os.getenv("RUN_ID")
          headers = {"Authorization": f"token {token}"}

          # Get all workflow runs for 'extractor.yml'
          runs_url = f"https://api.github.com/repos/{repo}/actions/workflows/extractor.yml/runs?status=completed&per_page=100"
          runs = requests.get(runs_url, headers=headers ).json().get("workflow_runs", [])

          os.makedirs("artifacts", exist_ok=True)
          downloaded_artifacts = 0

          print(f"Found {len(runs)} completed runs for the extractor workflow.")

          for run in runs:
              artifacts_url = run["artifacts_url"]
              artifacts = requests.get(artifacts_url, headers=headers).json().get("artifacts", [])
              for art in artifacts:
                  if "fmcsa-output-batch" in art["name"]:
                      print(f"Downloading artifact: {art['name']} from run ID {run['id']}")
                      dl_url = art["archive_download_url"]
                      r = requests.get(dl_url, headers=headers, timeout=60)
                      if r.status_code == 200:
                          with zipfile.ZipFile(BytesIO(r.content)) as zf:
                              zf.extractall("artifacts")
                              downloaded_artifacts += 1
                      else:
                          print(f"❌ Failed to download artifact {art['name']}. Status: {r.status_code}")

          print(f"✅ Downloaded {downloaded_artifacts} artifacts.")

          # Merge CSVs
          all_dfs = []
          csv_files = glob.glob("artifacts/**/*.csv", recursive=True)
          print(f"Found {len(csv_files)} CSV files to merge.")

          for file in csv_files:
              try:
                  df = pd.read_csv(file)
                  if not df.empty:
                      all_dfs.append(df)
              except Exception as e:
                  print(f"❌ Failed to read or process {file}: {e}")

          if all_dfs:
              final_df = pd.concat(all_dfs, ignore_index=True)
              print(f"Total rows before dropping duplicates: {len(final_df)}")
              # Duplicates ko email aur mcNumber ki bunyaad par drop karein
              final_df.drop_duplicates(subset=['email', 'mcNumber'], keep='first', inplace=True)
              print(f"Total rows after dropping duplicates: {len(final_df)}")
              
              os.makedirs("output", exist_ok=True)
              final_df.to_csv("output/merged_fmcsa_data.csv", index=False)
              print(f"✅ Final merged CSV created with {len(final_df)} unique rows.")
          else:
              print("⚠️ No data found in any CSVs. Nothing to merge.")
          EOF

      - name: Commit & Push merged file
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add output/merged_fmcsa_data.csv
          git commit -m "chore: Update merged FMCSA data" || echo "No changes to commit"
          git push

      - name: Trigger Cleanup
        if: always()
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "Triggering cleanup workflow..."
          gh workflow run fmcsa-cleanup.yml --ref main
